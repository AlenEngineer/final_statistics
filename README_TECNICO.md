# üìä README T√©cnico - An√°lisis de Regresi√≥n Completo

## üéØ Descripci√≥n del Proyecto

Este proyecto implementa un an√°lisis completo de regresi√≥n para el dataset `dropout.csv`, enfoc√°ndose en predecir la **"Probabilidad Retirarse (1-5)"** usando diferentes modelos de regresi√≥n estad√≠stica.

## üèóÔ∏è Arquitectura del C√≥digo

### üìÅ Estructura Principal

```python
analisis_regresion_completo.py
‚îú‚îÄ‚îÄ cargar_datos()           # Carga del dataset
‚îú‚îÄ‚îÄ preprocesar_datos()      # Codificaci√≥n de variables categ√≥ricas
‚îú‚îÄ‚îÄ regresion_multiple()     # Regresi√≥n m√∫ltiple (5 variables)
‚îú‚îÄ‚îÄ regresion_lineal_satisfaccion()  # Regresi√≥n lineal simple
‚îú‚îÄ‚îÄ regresion_curvilinea()   # Regresi√≥n exponencial
‚îú‚îÄ‚îÄ regresion_polinomial_grado4()    # Regresi√≥n polinomial
‚îú‚îÄ‚îÄ analisis_adicional()     # Correlaciones y estad√≠sticas
‚îú‚îÄ‚îÄ crear_grafico_comparativo()      # Gr√°fico comparativo
‚îî‚îÄ‚îÄ main()                   # Funci√≥n principal
```

## üîß Funciones Principales

### 1. **Carga y Preprocesamiento de Datos**

```python
def cargar_datos():
    """Carga el dataset dropout.csv."""
    try:
        df = pd.read_csv('dropout.csv')
        print(f"Dataset cargado exitosamente: {df.shape[0]} filas y {df.shape[1]} columnas")
        return df
    except FileNotFoundError:
        print("Error: No se encontr√≥ el archivo 'dropout.csv'")
        return None
```

```python
def preprocesar_datos(df):
    """Preprocesa los datos para el an√°lisis de regresi√≥n."""
    # Variables categ√≥ricas a codificar
    variables_categoricas = ['Situaci√≥n Laboral', 'Financiamiento Estudios', 
                           'Modalidad de Estudio', 'Considerado Dejar Carrera']
    
    # Codificar variables categ√≥ricas
    le = LabelEncoder()
    for col in variables_categoricas:
        if col in df_proc.columns:
            df_proc[col] = le.fit_transform(df_proc[col].astype(str))
            print(f"‚úì Codificada: {col}")
    
    return df_proc
```

### 2. **Regresi√≥n M√∫ltiple** (R¬≤ = 0.598)

```python
def regresion_multiple(df):
    """Implementa regresi√≥n m√∫ltiple para predecir Probabilidad Retirarse."""
    
    # Variables predictoras (5 variables)
    variables_predictoras = ['√çndice Acad√©mico', 'Nivel Estr√©s Acad√©mico (1-5)', 
                           'Satisfacci√≥n Carrera (1-5)', 'Situaci√≥n Laboral', 
                           'Considerado Dejar Carrera']
    
    # Preparar datos
    X = df[variables_disponibles].values
    y = df['Probabilidad Retirarse (1-5)'].values
    
    # Entrenar modelo
    model = LinearRegression()
    model.fit(X, y)
    
    # Predicciones y m√©tricas
    y_pred = model.predict(X)
    r2 = r2_score(y, y_pred)
    mse = mean_squared_error(y, y_pred)
    
    # Crear gr√°fico observado vs predicho
    plt.scatter(y, y_pred, alpha=0.6, s=60, color='blue', label='Datos')
    plt.plot([min_val, max_val], [min_val, max_val], 'r-', linewidth=2, 
             label='Predicci√≥n Perfecta')
```

**Resultados:**
- **R¬≤ = 0.598** (Mejor modelo)
- **MSE = 0.458**
- **Coeficientes:** Satisfacci√≥n (-0.478), Estr√©s (0.274), √çndice (-0.156)

### 3. **Regresi√≥n Lineal Simple** (R¬≤ = 0.496)

```python
def regresion_lineal_satisfaccion(df):
    """Regresi√≥n lineal: Satisfacci√≥n Carrera vs Probabilidad Retirarse."""
    
    # Variables
    X_satisfaccion = df[['Satisfacci√≥n Carrera (1-5)']].values
    y_satisfaccion = df['Probabilidad Retirarse (1-5)'].values
    
    # Modelo y m√©tricas
    model_satisfaccion = LinearRegression()
    model_satisfaccion.fit(X_satisfaccion, y_satisfaccion)
    
    # Estad√≠sticas
    corr_satisfaccion, pval_satisfaccion = pearsonr(X_satisfaccion.flatten(), y_satisfaccion)
    r2_satisfaccion = r2_score(y_satisfaccion, y_pred_satisfaccion)
    
    # Gr√°fico con seaborn
    sns.regplot(x='Satisfacci√≥n Carrera (1-5)', y='Probabilidad Retirarse (1-5)', 
                data=df, ci=None, line_kws={'color':'red', 'linewidth': 2})
```

**Resultados:**
- **R¬≤ = 0.496** (2do mejor predictor)
- **r = -0.704** (Correlaci√≥n negativa fuerte)
- **p = 6.42e-13** (Muy significativo)

### 4. **Regresi√≥n Curvil√≠nea (Exponencial)** (R¬≤ = 0.443)

```python
def regresion_curvilinea(df):
    """Regresi√≥n exponencial: Estr√©s Acad√©mico vs Probabilidad Retirarse."""
    
    # Funci√≥n exponencial: y = a * exp(b * x)
    def exp_func(x, a, b):
        return a * np.exp(b * x)
    
    # Variables
    X_curve = df['Nivel Estr√©s Acad√©mico (1-5)'].values
    y_curve = df['Probabilidad Retirarse (1-5)'].values
    
    # Ajuste exponencial
    params, cov = curve_fit(exp_func, X_curve, y_curve, p0=(1, 0.1))
    y_pred_curve = exp_func(X_curve, *params)
    r2_curve = r2_score(y_curve, y_pred_curve)
    
    # L√≠nea suave para visualizaci√≥n
    X_smooth = np.linspace(X_curve.min(), X_curve.max(), 100)
    y_smooth = exp_func(X_smooth, *params)
    plt.plot(X_smooth, y_smooth, color='red', linewidth=2, label='Ajuste Exponencial')
```

**Resultados:**
- **R¬≤ = 0.443** (3er mejor modelo)
- **Par√°metros:** a=0.571, b=0.361
- **Relaci√≥n:** Exponencial creciente

### 5. **Regresi√≥n Polinomial Grado 4** (R¬≤ = 0.222)

```python
def regresion_polinomial_grado4(df):
    """Regresi√≥n polinomial: √çndice Acad√©mico vs Probabilidad Retirarse."""
    
    # Funci√≥n polinomial: y = a*x^4 + b*x^3 + c*x^2 + d*x + e
    def poly4_func(x, a, b, c, d, e):
        return a * np.power(x, 4) + b * np.power(x, 3) + c * np.power(x, 2) + d * x + e
    
    # Variables
    X_poly = df['√çndice Acad√©mico'].values
    y_poly = df['Probabilidad Retirarse (1-5)'].values
    
    # Ajuste polinomial
    params, cov = curve_fit(poly4_func, X_poly, y_poly, p0=(0.5, -2, 3, -1, 4))
    y_pred_poly = poly4_func(X_poly, *params)
    r2_poly = r2_score(y_poly, y_pred_poly)
    
    # Comparaci√≥n con otros modelos
    models = [("Polinomial Grado 4", r2_poly), ("Polinomial Grado 2", r2_poly2), 
             ("Lineal", r2_simple)]
    best_model = max(models, key=lambda x: x[1])
```

**Resultados:**
- **R¬≤ = 0.222** (Mejor entre polinomiales)
- **Funci√≥n:** y = 4.200x‚Å¥ + -34.850x¬≥ + 103.725x¬≤ + -130.637x + 60.467
- **Comparaci√≥n:** Grado 4 > Grado 2 > Lineal

## üìà Conceptos Estad√≠sticos Clave

### **M√©tricas de Evaluaci√≥n**

```python
# R¬≤ (Coeficiente de determinaci√≥n)
r2 = r2_score(y_true, y_pred)

# MSE (Error cuadr√°tico medio)
mse = mean_squared_error(y_true, y_pred)

# Correlaci√≥n de Pearson
corr, pval = pearsonr(x, y)

# Desviaci√≥n est√°ndar
std = np.std(x)
```

### **Tipos de Regresi√≥n Implementados**

| Modelo | R¬≤ | Variables | Comentario |
|--------|----|-----------|------------|
| **M√∫ltiple** | 0.598 | 5 variables | Mejor modelo |
| **Lineal Satisfacci√≥n** | 0.496 | 1 variable | 2do mejor |
| **Curvil√≠nea** | 0.443 | 1 variable | Exponencial |
| **Polinomial Grado 4** | 0.222 | 1 variable | Complejo |

### **Ranking de Predictores**

1. **Satisfacci√≥n Carrera** (r = -0.704, p = 6.42e-13)
2. **Estr√©s Acad√©mico** (r = 0.645, p = 1.82e-10)
3. **√çndice Acad√©mico** (r = -0.365, p = 0.00102)

## üé® Visualizaci√≥n de Datos

### **Gr√°ficos Generados**

```python
# Configuraci√≥n de gr√°ficos
plt.figure(figsize=(12, 8))
plt.title('T√≠tulo del Gr√°fico\nR¬≤ = {r2:.3f} | r = {corr:.3f}')
plt.xlabel('Variable X')
plt.ylabel('Variable Y')
plt.grid(True, alpha=0.3)

# Guardar gr√°fico
plt.savefig('nombre_grafico.png', dpi=300, bbox_inches='tight')
```

**Archivos PNG generados:**
- `regresion_multiple.png` (350KB)
- `regresion_lineal_satisfaccion.png` (253KB)
- `regresion_curvilinea.png` (218KB)
- `regresion_polinomial_grado4.png` (285KB)
- `comparacion_regresiones.png` (536KB)

## üîç An√°lisis de Correlaciones

```python
def analisis_adicional(df):
    """An√°lisis de correlaciones relevantes."""
    
    print("Resumen de correlaciones relevantes:")
    for col in ['√çndice Acad√©mico', 'Nivel Estr√©s Acad√©mico (1-5)', 'Satisfacci√≥n Carrera (1-5)']:
        corr, pval = pearsonr(df[col], df['Probabilidad Retirarse (1-5)'])
        print(f"  {col} vs Probabilidad Retirarse: r={corr:.3f}, p={pval:.3g}")
    
    print("\nDesviaciones est√°ndar de variables clave:")
    for col in ['√çndice Acad√©mico', 'Nivel Estr√©s Acad√©mico (1-5)', 'Satisfacci√≥n Carrera (1-5)']:
        print(f"  {col}: {np.std(df[col]):.3f}")
```

## üöÄ Ejecuci√≥n del Script

```bash
# Instalar dependencias
uv add pandas numpy matplotlib seaborn scipy scikit-learn

# Ejecutar an√°lisis completo
uv run python analisis_regresion_completo.py
```

## üìä Resultados Principales

### **Modelo √ìptimo: Regresi√≥n M√∫ltiple**
- **R¬≤ = 0.598** (59.8% de varianza explicada)
- **5 variables predictoras**
- **MSE = 0.458**

### **Mejor Predictor Individual: Satisfacci√≥n Carrera**
- **R¬≤ = 0.496** (49.6% de varianza explicada)
- **Correlaci√≥n negativa fuerte** (r = -0.704)
- **Muy significativo** (p = 6.42e-13)

### **Interpretaci√≥n de Resultados**
1. **Satisfacci√≥n con la carrera** es el factor m√°s importante para predecir la probabilidad de retiro
2. **Estr√©s acad√©mico** tiene una relaci√≥n exponencial con la probabilidad de retiro
3. **√çndice acad√©mico** tiene una relaci√≥n polinomial compleja
4. El **modelo m√∫ltiple** combina todos los factores de manera √≥ptima

## üõ†Ô∏è Dependencias

```toml
# pyproject.toml
[project]
dependencies = [
    "pandas>=2.0.0",
    "numpy>=1.24.0",
    "matplotlib>=3.7.0",
    "seaborn>=0.12.0",
    "scipy>=1.10.0",
    "scikit-learn>=1.3.0"
]
```

## üìù Notas T√©cnicas

- **Codificaci√≥n de variables categ√≥ricas** usando `LabelEncoder`
- **Manejo de errores** en ajustes de curvas con `try-except`
- **Visualizaci√≥n profesional** con estad√≠sticas integradas
- **Comparaci√≥n sistem√°tica** de modelos
- **Documentaci√≥n completa** de resultados

---

**Autor:** Asistente IA  
**Fecha:** 2024  
**Dataset:** dropout.csv (78 filas, 20 columnas) 